{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5nagEQZrLbp",
        "outputId": "ab5257e2-0d17-4887-a90d-fc0b41784461"
      },
      "outputs": [],
      "source": [
        "#!conda create --name pem python=3.10 -y\n",
        "#!conda activate pem\n",
        "\n",
        "#!conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.3 -c pytorch\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "!git clone https://github.com/NiccoloCavagnero/PEM.git\n",
        "import os\n",
        "\n",
        "# Ottieni il percorso corrente\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "# Cambia directory\n",
        "os.chdir('PEM')\n",
        "\n",
        "# Ottieni il nuovo percorso\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijx_M8RE_GSl"
      },
      "source": [
        "Scarico il dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oga3B2sN9CoJ",
        "outputId": "8cce94cf-a6e3-47dd-a8ef-97cd3501351c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Ottieni il percorso corrente\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "# Cambia directory\n",
        "os.chdir('datasets')\n",
        "\n",
        "# Ottieni il nuovo percorso\n",
        "current_path = os.getcwd()\n",
        "print(current_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfjMTINy7fPF",
        "outputId": "5f54e01b-5d81-4a64-9261-1ef64ccc31a9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "# Funzione per scaricare un file e decomprimerlo\n",
        "def download_and_extract(url, zip_path, extract_path):\n",
        "    # Se il file ZIP esiste già, non scaricarlo di nuovo\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Downloading {url} to {zip_path}...\")\n",
        "        # Scarica il file usando curl\n",
        "        try:\n",
        "            subprocess.run([\"curl\", \"-L\", \"-o\", zip_path, url], check=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return\n",
        "\n",
        "    # Crea la directory di estrazione se non esiste\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # Estrai il file ZIP\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"Extracted files to {extract_path}\")\n",
        "    except zipfile.BadZipFile as e:\n",
        "        print(f\"Error extracting {zip_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Elenca i file estratti\n",
        "    extracted_files = os.listdir(extract_path)\n",
        "    print(f\"Extracted files in {extract_path}:\", extracted_files)\n",
        "\n",
        "# Percorsi principali\n",
        "base_dir = os.path.expanduser(\"loveDa_zipped\")\n",
        "\n",
        "# Crea la directory base se non esiste\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Dataset Val\n",
        "val_url = \"https://zenodo.org/records/5706578/files/Val.zip?download=1\"\n",
        "val_zip_path = os.path.join(base_dir, \"Val.zip\")\n",
        "val_extract_path = os.path.join(base_dir, \"\")\n",
        "download_and_extract(val_url, val_zip_path, val_extract_path)\n",
        "\n",
        "# Dataset Train\n",
        "train_url = \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n",
        "train_zip_path = os.path.join(base_dir, \"Train.zip\")\n",
        "train_extract_path = os.path.join(base_dir, \"\")\n",
        "download_and_extract(train_url, train_zip_path, train_extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4CM-kFZ_L0K"
      },
      "source": [
        "Formatto il dataset come richiesto\n",
        "LOVEDA\n",
        "├───Test\n",
        "│   ├───Rural\n",
        "│   │   └───images_png\n",
        "│   └───Urban\n",
        "│       └───images_png\n",
        "├───Train\n",
        "│   ├───Rural\n",
        "│   │   ├───images_png\n",
        "│   │   └───masks_png\n",
        "│   └───Urban\n",
        "│       ├───images_png\n",
        "│       └───masks_png\n",
        "└───Val\n",
        "    ├───Rural\n",
        "    │   ├───images_png\n",
        "    │   └───masks_png\n",
        "    └───Urban\n",
        "        ├───images_png\n",
        "        └───masks_png\n",
        "\n",
        "in\n",
        "loveDa\n",
        "    ├───train\n",
        "    │   ├───annotations // mask contenute in Train/Urban/mask_png\n",
        "    │   └───images // immagini contenute in Train/Urban/images_png\n",
        "    ├───val\n",
        "    │   ├───annotations  // mask contenute in Val/Urban/mask_png\n",
        "    │   └───images  // immagini contenute in Val/Urban/images_png\n",
        "    └───val_rural\n",
        "        ├───annotations  // mask contenute in Val/Rural/mask_png\n",
        "        └───images   // immagini contenute in Val/Rural/images_png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J3XyHwUw86xv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def reorganize_loveda(source_dir, target_dir):\n",
        "    # Crea le directory principali\n",
        "    splits = {\n",
        "        'train': ['Urban'],\n",
        "        'val': ['Urban'],\n",
        "        'val_rural': ['Rural']\n",
        "    }\n",
        "\n",
        "    for split, areas in splits.items():\n",
        "        # Crea le sottodirectory per ogni split\n",
        "        (Path(target_dir) / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "        (Path(target_dir) / split / 'annotations').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for area in areas:\n",
        "            # Source paths\n",
        "            if split == 'val_rural':\n",
        "                src_split = 'Val'\n",
        "            elif split == 'val':\n",
        "                src_split = 'Val'\n",
        "            else:\n",
        "                src_split = 'Train'\n",
        "\n",
        "            src_img = Path(source_dir) / src_split / area / 'images_png'\n",
        "            src_mask = Path(source_dir) / src_split / area / 'masks_png'\n",
        "\n",
        "            # Target paths\n",
        "            dst_img = Path(target_dir) / split / 'images'\n",
        "            dst_mask = Path(target_dir) / split / 'annotations'\n",
        "\n",
        "            # Copia le immagini\n",
        "            if src_img.exists():\n",
        "                for img in src_img.glob('*'):\n",
        "                    shutil.copy2(img, dst_img / img.name)\n",
        "\n",
        "            # Copia le maschere se esistono\n",
        "            if src_mask.exists():\n",
        "                for mask in src_mask.glob('*'):\n",
        "                    shutil.copy2(mask, dst_mask / mask.name)\n",
        "\n",
        "    # Elimina la directory sorgente dopo la copia\n",
        "    shutil.rmtree(source_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_directory = \"loveDa_zipped\"  # Directory sorgente\n",
        "    target_directory = \"loveDa\"  # Directory destinazione\n",
        "\n",
        "    reorganize_loveda(source_directory, target_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXFduVLC9pWG",
        "outputId": "033d71c7-04d2-42b1-8853-fcf5168cb61d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ottieni il percorso corrente\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "# Cambia directory\n",
        "os.chdir('..')\n",
        "\n",
        "# Ottieni il nuovo percorso\n",
        "current_path = os.getcwd()\n",
        "print(current_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MZBy40u9vAy"
      },
      "source": [
        "inserisci le cartelle in config, lovaDa.py in dataset e  \n",
        "# semantic segmentation\n",
        "        if evaluator_type in [\"sem_seg\", \"ade20k_panoptic_seg\", \"loveda_seg\"]:\n",
        "            evaluator_list.append(\n",
        "                SemSegEvaluator(\n",
        "                    dataset_name,\n",
        "                    distributed=True,\n",
        "                    output_dir=output_folder,\n",
        "                )\n",
        "            )\n",
        "\n",
        "in train_net.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_ARX9Wa-hog",
        "outputId": "78314ee4-1974-4b77-ea43-a7e20ba3d7b3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "# Ottieni il percorso corrente\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "\n",
        "# Crea la directory per i modelli pre-addestrati\n",
        "os.makedirs('pretrained_models', exist_ok=True)\n",
        "os.chdir('pretrained_models')\n",
        "\n",
        "# Download dei modelli usando gdown\n",
        "\n",
        "!gdown 1DFoXcV42zy-apUcMh5P8WhsXMRJofgl8\n",
        "!gdown 1Y5belNkq3Dn-EYgSKY-ICiPsN4TZXoXO\n",
        "\n",
        "# Converti i modelli\n",
        "!python ../tools/convert-pretrained-stdc-model-to-d2.py STDCNet813M_73.91.tar STDC1.pkl\n",
        "!python ../tools/convert-pretrained-stdc-model-to-d2.py STDCNet1446_76.47.tar STDC2.pkl\n",
        "\n",
        "# Torna alla directory principale\n",
        "os.chdir('..')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAvbeA-n-k-W",
        "outputId": "b0e779f9-0248-447e-bdf9-17cf26015ab1"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "# Collect garbage\n",
        "gc.collect()\n",
        "# Check allocated memory\n",
        "print(torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
        "# Check cached memory\n",
        "print(torch.cuda.memory_cached() / 1024**2, \"MB\")\n",
        "\n",
        "\n",
        "!python train_net.py \\\n",
        "       --config-file ./configs/loveDa/semantic-segmentation/pem_R50_bs32_90k.yaml \\\n",
        "       --num-gpus 1 \\\n",
        "       DATALOADER.NUM_WORKERS 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
